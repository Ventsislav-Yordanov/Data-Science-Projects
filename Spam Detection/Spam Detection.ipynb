{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection\n",
    "## Author: Ventsislav Yordanov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/2j5RA3SioKdck/giphy.gif\" style=\"height:400px\"/>\n",
    "Image Source: https://media.giphy.com/media/2j5RA3SioKdck/giphy.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Feature extraction, model evaluation and hyperparemter optimization\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "Can we use this dataset to build a protection model that will accurately classify which messages are spam? This application is widely used from the email service providers like Gmail, Yahoo, and so on.\n",
    "\n",
    "<img src=\"https://i.gifer.com/Ou1t.gif\" style=\"height:400px\"/>\n",
    "Image Source: https://i.gifer.com/Ou1t.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged according being ham (legitimate) or spam.\n",
    "\n",
    "Source: https://www.kaggle.com/uciml/sms-spam-collection-dataset/home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam.csv\", encoding = \"latin-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Dataset\n",
    "We can see that we have 5 columns with very confusing names. However, it's easy to see that the first column contains the target. The second one contains the message text. The other columns may be some additional notes. Let's explore them a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, ' PO Box 5249',\n",
       "       ' the person is definitely special for u..... But if the person is so special',\n",
       "       ' HOWU DOIN? FOUNDURSELF A JOBYET SAUSAGE?LOVE JEN XXX\\\\\"\"',\n",
       "       ' wanted to say hi. HI!!!\\\\\" Stop? Send STOP to 62468\"',\n",
       "       'this wont even start........ Datz confidence..\"', 'GN',\n",
       "       '.;-):-D\"',\n",
       "       'just been in bedbut mite go 2 thepub l8tr if uwana mt up?loads a luv Jenxxx.\\\\\"\"',\n",
       "       ' bt not his girlfrnd... G o o d n i g h t . . .@\"',\n",
       "       ' I\\'ll come up\"',\n",
       "       ' don\\'t miss ur best life for anything... Gud nyt...\"',\n",
       "       ' just as a shop has to give a guarantee on what they sell. B. G.\"',\n",
       "       ' But at d end my love compromised me for everything:-(\\\\\".. Gud mornin:-)\"',\n",
       "       ' the toughest is acting Happy with all unspoken pain inside..\\\\\"\"',\n",
       "       ' smoke hella weed\\\\\"\"', '\\\\\" not \\\\\"what i need to do.\\\\\"\"',\n",
       "       'JUST GOT PAYED2DAY & I HAVBEEN GIVEN Aå£50 PAY RISE 4MY WORK & HAVEBEEN MADE PRESCHOOLCO-ORDINATOR 2I AM FEELINGOOD LUV\\\\\"\"',\n",
       "       ' justthought iåÕd sayhey! how u doin?nearly the endof me wk offdam nevamind!We will have 2Hook up sn if uwant m8? loveJen x.\\\\\"\"',\n",
       "       'JUST REALLYNEED 2DOCD.PLEASE DONTPLEASE DONTIGNORE MYCALLS',\n",
       "       'u hav2hear it!c u sn xxxx\\\\\"\"', \" I don't mind\",\n",
       "       ' Dont Come Near My Body..!! Bcoz My Hands May Not Come 2 Wipe Ur Tears Off That Time..!Gud ni8\"',\n",
       "       \"Well there's still a bit left if you guys want to tonight\",\n",
       "       ' but dont try to prove\\\\\" ..... Gud mrng...\"',\n",
       "       ' SHE SHUDVETOLD U. DID URGRAN KNOW?NEWAY',\n",
       "       ' but watever u shared should be true\\\\\"....\"',\n",
       "       ' like you are the KING\\\\\"...! OR \\\\\"Walk like you Dont care',\n",
       "       ' HAD A COOL NYTHO', ' PO Box 1146 MK45 2WT (2/3)\"',\n",
       "       ' \\\\\"It is d wonderful fruit that a tree gives when it is being hurt by a stone.. Good night......\"',\n",
       "       ' we made you hold all the weed\\\\\"\"',\n",
       "       ' but dont try to prove it..\\\\\" .Gud noon....\"',\n",
       "       ' its a miracle to Love a person who can\\'t Love anyone except U...\\\\\" Gud nyt...\"',\n",
       "       ' Gud night....\"',\n",
       "       ' that\\'s the tiny street where the parking lot is\"',\n",
       "       'PROBPOP IN & CU SATTHEN HUNNY 4BREKKIE! LOVE JEN XXX. PSXTRA LRG PORTIONS 4 ME PLEASE \\\\\"\"',\n",
       "       ' hopeSo hunny. i amnow feelin ill & ithink i may have tonsolitusaswell! damn iam layin in bedreal bored. lotsof luv me xxxx\\\\\"\"',\n",
       "       ' GOD said',\n",
       "       ' always give response 2 who cares 4 U\\\\\"... Gud night..swt dreams..take care\"',\n",
       "       ' HOPE UR OK... WILL GIVE U A BUZ WEDLUNCH. GO OUTSOMEWHERE 4 ADRINK IN TOWN..CUD GO 2WATERSHD 4 A BIT? PPL FROMWRK WILL BTHERE. LOVE PETEXXX.\\\\\"\"',\n",
       "       ' b\\'coz nobody will fight for u. Only u &amp; u have to fight for ur self &amp; win the battle. -VIVEKANAND- G 9t.. SD..\"',\n",
       "       'DEVIOUSBITCH.ANYWAY',\n",
       "       ' ENJOYIN INDIANS AT THE MO..yeP. SaLL gOoD HehE ;> hows bout u shexy? Pete Xx\\\\\"\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(df.iloc[:, 2].notna()))\n",
    "df.iloc[:, 2].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, ' MK17 92H. 450Ppw 16\"', ' why to miss them', 'GE',\n",
       "       'U NO THECD ISV.IMPORTANT TOME 4 2MORO\\\\\"\"',\n",
       "       'i wil tolerat.bcs ur my someone..... But',\n",
       "       ' ILLSPEAK 2 U2MORO WEN IM NOT ASLEEP...\\\\\"\"',\n",
       "       'whoever is the KING\\\\\"!... Gud nyt\"', ' TX 4 FONIN HON',\n",
       "       ' \\\\\"OH No! COMPETITION\\\\\". Who knew', 'IåÕL CALL U\\\\\"\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(df.iloc[:, 3].notna()))\n",
    "df.iloc[:, 3].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, ' just Keep-in-touch\\\\\" gdeve..\"', 'GNT:-)\"',\n",
       "       ' Never comfort me with a lie\\\\\" gud ni8 and sweet dreams\"',\n",
       "       ' CALL 2MWEN IM BK FRMCLOUD 9! J X\\\\\"\"',\n",
       "       ' one day these two will become FREINDS FOREVER!\"'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(df.iloc[:, 4].notna()))\n",
    "df.iloc[:, 4].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems that these column contains some additional comments about the messages. However, they contain only a few values and there is no documentation about them in the source. I think it's safe to remove them and try to build our machine learning model only on the message text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"v1\", \"v2\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are still with confusing names, let's rename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [\"class\", \"message\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the messages\n",
    "We want to keep only the important and useful words. To achieve these we will follow the steps:\n",
    "1. **Keep only the words** in the message\n",
    "2. Transform all words in **lower case**. We want **\"Love\"** and **\"love\"** to mean the same thing.\n",
    "3. Remove all **stop words**. Stop words usually refers to the most common words in a language, for example: **\"the\", \"a\", \"is\", etc.** We don't need these words. They don't give us any insight.\n",
    "4. Perform **stemming**. Stemming is a process in which we get the **root of the words**. We want all the different versions of the same word to be presented in one word. They all mean the same thing. Example: **\"love\", \"loving\", \"lovely\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\venci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the last available version of the stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(message):\n",
    "    \"\"\"\n",
    "    Receives a raw message and clean it using the following steps:\n",
    "    1. Remove all non-words in the message\n",
    "    2. Transform the message in lower case\n",
    "    3. Remove all stop words\n",
    "    4. Perform stemming\n",
    "\n",
    "    Args:\n",
    "        message: the raw message\n",
    "    Returns:\n",
    "        a clean message using the mentioned steps above.\n",
    "    \"\"\"\n",
    "    \n",
    "    message = re.sub(\"[^A-Za-z]\", \" \", message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "    stemmer = PorterStemmer()\n",
    "    message = [stemmer.stem(word) for word in message if word not in set(stopwords.words(\"english\"))]\n",
    "    message = \" \".join(message)\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "go jurong point crazi avail bugi n great world la e buffet cine got amor wat\n"
     ]
    }
   ],
   "source": [
    "# Testing how our function works\n",
    "message = df.message[0]\n",
    "print(message)\n",
    "\n",
    "message = clean_message(message)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(df)):\n",
    "    message = clean_message(df.message[i])\n",
    "    corpus.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri wkli comp win fa cup final tkt st may text fa receiv entri question std txt rate c appli',\n",
       " 'u dun say earli hor u c alreadi say',\n",
       " 'nah think goe usf live around though']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Let's see what part of the messages are **spam** and what are legitimate (**ham**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.59\n",
      "13.41\n"
     ]
    }
   ],
   "source": [
    "print(round(sum(df[\"class\"] == \"ham\") / len(df) * 100, 2))\n",
    "print(round(sum(df[\"class\"] == \"spam\") / len(df) * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Bag of Words Model\n",
    "Bag of Words model is a very popular **NLP model** used to **preprocess the texts** to classify before fitting the classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 6221)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "features = count_vectorizer.fit_transform(corpus).toarray()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'spam', 'ham', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df[\"class\"].values\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into Test and Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, \n",
    "    test_size = 0.20, stratify = labels, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aah', 'aaniy', 'aaooooright', 'aathi', 'ab', 'abbey', 'abdomen', 'abeg', 'abel']\n",
      "['zero', 'zf', 'zhong', 'zindgi', 'zoe', 'zogtoriu', 'zoom', 'zouk', 'zs', 'zyada']\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names()[:10])\n",
    "print(count_vectorizer.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Multinomial Naive Bayes Classifier.\n",
    "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work. That's why we're going to try this model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.9780143003889666 std: 0.010891013474211515\n"
     ]
    }
   ],
   "source": [
    "k_fold = StratifiedKFold(n_splits = 10)\n",
    "scores = cross_val_score(nb_classifier, features_train, labels_train, cv = k_fold)\n",
    "print(\"mean:\" , scores.mean(), \"std:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775784753363229"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.fit(features_train, labels_train)\n",
    "labels_predicted = nb_classifier.predict(features_test)\n",
    "accuracy_score(labels_test, labels_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[950,  16],\n",
       "       [  9, 140]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labels_test, labels_predicted, labels = [\"ham\", \"spam\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venci\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\venci\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\venci\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\venci\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\venci\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\venci\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\venci\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\venci\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\venci\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\venci\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV params {'alpha': 0.8}\n",
      "Best CV accuracy 0.9771146511106125\n",
      "Test accuracy of best grid search hypers: 0.9775784753363229\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits = 10)\n",
    "parameters = {\"alpha\": np.arange(0, 1, 0.1)}\n",
    "searcher = GridSearchCV(MultinomialNB(), param_grid = parameters, cv = kfold)\n",
    "searcher.fit(features_train, labels_train)\n",
    "best_multinomial_nb = searcher.best_estimator_\n",
    "\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning: check me later\n",
    "There is something strange here after the fine-tuning we have a little bit more bad accuracy.\n",
    "**Is this the accuracy paradox?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores definitions\n",
    "To choose our model we'll use the accuracy, recall, precision, and f1 score. Here are some definitions for this metrics:\n",
    "* Accuracy: Overall, how often is the classifier correct?\n",
    "* Recall: When it's actually yes, how often does it predict yes?\n",
    "* Precision: When it predicts yes, how often is it correct?\n",
    "* F1 score: can be interpreted as a weighted average of the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9775784753363229\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.98      0.99       966\n",
      "       spam       0.89      0.95      0.92       149\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_predicted = best_multinomial_nb.predict(features_test)\n",
    "print(\"Accuracy Score:\", accuracy_score(labels_test, labels_predicted))\n",
    "print(classification_report(labels_test, labels_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Classifiers: Logistic Regression, Decision Tree, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "Mean score: 0.9822738771421191 Std: 0.005258459452394159\n",
      "\n",
      "Test Accuracy Score: 0.9829596412556054\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99       966\n",
      "       spam       1.00      0.87      0.93       149\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1115\n",
      "\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "Mean score: 0.9744228142588096 Std: 0.009202399326537672\n",
      "\n",
      "Test Accuracy Score: 0.9739910313901345\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      0.99      0.99       966\n",
      "       spam       0.95      0.85      0.90       149\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1115\n",
      "\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "Mean score: 0.9733027434649845 Std: 0.007789228885206418\n",
      "\n",
      "Test Accuracy Score: 0.9668161434977578\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.96      1.00      0.98       966\n",
      "       spam       1.00      0.75      0.86       149\n",
      "\n",
      "avg / total       0.97      0.97      0.96      1115\n",
      "\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "models = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier()]\n",
    "for model in models:\n",
    "    model.fit(features_train, labels_train)\n",
    "\n",
    "    scores = cross_val_score(model, features_train, labels_train, cv = kfold)\n",
    "    print(type(model))\n",
    "    print(\"Mean score:\" , scores.mean(), \"Std:\", scores.std())\n",
    "    print()\n",
    "\n",
    "    predictions = model.predict(features_test)\n",
    "    accuracy_score(labels_test, predictions)\n",
    "\n",
    "    labels_predicted = model.predict(features_test)\n",
    "    print(\"Test Accuracy Score:\", accuracy_score(labels_test, labels_predicted))\n",
    "    print(classification_report(labels_test, labels_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Well, it is controversial which is the best model. It depends on what's important for our spam detection. Personally, I think that the precision metric for the spam class is very important, but the recall is also important. In such a case when we don't know which classifier to choose. We can use the best f1 score. If some classifiers have exactly the same f1 score, we can choose the simpler one. So, if we follow this rule, we can see that the logistic regression give us the best score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources:\n",
    "* https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
    "* https://en.wikipedia.org/wiki/Natural_language_processing\n",
    "* https://en.wikipedia.org/wiki/Stop_words\n",
    "* https://en.wikipedia.org/wiki/Stemming\n",
    "* http://scikit-learn.org/stable/index.html\n",
    "* https://stats.stackexchange.com/questions/250273/benefits-of-stratified-vs-random-sampling-for-generating-training-data-in-classi/250742#250742\n",
    "* https://stats.stackexchange.com/questions/117643/why-use-stratified-cross-validation-why-does-this-not-damage-variance-related-b/117649#117649?newreg=2a9d984517504dcbbf55fda2f11489b7\n",
    "* https://stats.stackexchange.com/questions/49540/understanding-stratified-cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "* Try different values for test_size in the \"train_test_split\" function\n",
    "* Try CountVectorizer with some values for the \"max_features\" parameter\n",
    "* Use pickle to save the trained classifiers\n",
    "\n",
    "# Future Ideas\n",
    "* Compare more classifiers\n",
    "* Try to use TFIDF and compare the results\n",
    "* Try to use Dimensionality Reduction\n",
    "\n",
    "# Notes\n",
    "* Reduce the number of the features, because they are too many now and this may leads to overfitting\n",
    "* Add more data to avoid high bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
